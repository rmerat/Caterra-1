@INPROCEEDINGS{8730214,
  author={Arango Quiroz, Ricardo A. and Pereira Guidotti, Fernada and Bedoya, Albeiro Espinosa},
  booktitle={2019 XXII Symposium on Image, Signal Processing and Artificial Vision (STSIVA)}, 
  title={A method for automatic identification of crop lines in drone images from a mango tree plantation using segmentation over YCrCb color space and Hough transform}, 
  year={2019},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/STSIVA.2019.8730214}}

@article{VEGEIDX,
title = {Verification of color vegetation indices for automated crop imaging applications},
journal = {Computers and Electronics in Agriculture},
volume = {63},
number = {2},
pages = {282-293},
year = {2008},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2008.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0168169908001063},
author = {George E. Meyer and João Camargo Neto},
keywords = {Color images, Machine vision, Plant, Residue, Soil, Vegetation index}
}

@Article{ExgreenIdx,
AUTHOR = {Ronchetti, Giulia and Mayer, Alice and Facchi, Arianna and Ortuani, Bianca and Sona, Giovanna},
TITLE = {Crop Row Detection through UAV Surveys to Optimize On-Farm Irrigation Management},
JOURNAL = {Remote Sensing},
VOLUME = {12},
YEAR = {2020},
NUMBER = {12},
ARTICLE-NUMBER = {1967},
URL = {https://www.mdpi.com/2072-4292/12/12/1967},
ISSN = {2072-4292},
ABSTRACT = {Climate change and competition among water users are increasingly leading to a reduction of water availability for irrigation; at the same time, traditionally non-irrigated crops require irrigation to achieve high quality standards. In the context of precision agriculture, particular attention is given to the optimization of on-farm irrigation management, based on the knowledge of within-field variability of crop and soil properties, to increase crop yield quality and ensure an efficient water use. Unmanned Aerial Vehicle (UAV) imagery is used in precision agriculture to monitor crop variability, but in the case of row-crops, image post-processing is required to separate crop rows from soil background and weeds. This study focuses on the crop row detection and extraction from images acquired through a UAV during the cropping season of 2018. Thresholding algorithms, classification algorithms, and Bayesian segmentation are tested and compared on three different crop types, namely grapevine, pear, and tomato, for analyzing the suitability of these methods with respect to the characteristics of each crop. The obtained results are promising, with overall accuracy greater than 90% and producer&rsquo;s accuracy over 85% for the class &ldquo;crop canopy&rdquo;. The methods&rsquo; performances vary according to the crop types, input data, and parameters used. Some important outcomes can be pointed out from our study: NIR information does not give any particular added value, and RGB sensors should be preferred to identify crop rows; the presence of shadows in the inter-row distances may affect crop detection on vineyards. Finally, the best methodologies to be adopted for practical applications are discussed.},
DOI = {10.3390/rs12121967}
}

@article{CIVE,
  title={WEED DETECTION IN WHEAT CROP USING UAV for PRECISION AGRICULTURE},
  author={Ahmed Mateen and Zhu Qingsheng},
  journal={Pakistan Journal of Agricultural Sciences},
  year={2019}
}

@INPROCEEDINGS{NDVI,

  author={Katari, Sushma and Bhowmik, Tapan K and Nair, Shabarinath S and S, Aravind and Nayak, Akasha R and Pankajakshan, Praveen},

  booktitle={IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium}, 

  title={Crop Phenology Stage Forecasting and Detection Using NDVI Time-Series and LSTM}, 

  year={2022},

  volume={},

  number={},

  pages={6264-6267},

  doi={10.1109/IGARSS46834.2022.9883407}}



@ARTICLE{CNN1,
  author={Bah, Mamadou Dian and Hafiane, Adel and Canals, Raphael},
  journal={IEEE Access}, 
  title={CRowNet: Deep Network for Crop Row Detection in UAV Images}, 
  year={2020},
  volume={8},
  number={},
  pages={5189-5200},
  doi={10.1109/ACCESS.2019.2960873}}

@MASTERSTHESIS{thesisDOHA,
  AUTHOR =       {Rashed Mohammad Doha},
  TITLE =        {Master of Science in Mechanical Engineering},
  SCHOOL =       {Purdue University},
  YEAR =         {2022},
  month =        {may},
  }


  
@Article{LeastSquares,
AUTHOR = {Chen, Pengfei and Ma, Xiao and Wang, Fangyong and Li, Jing},
TITLE = {A New Method for Crop Row Detection Using Unmanned Aerial Vehicle Images},
JOURNAL = {Remote Sensing},
VOLUME = {13},
YEAR = {2021},
NUMBER = {17},
ARTICLE-NUMBER = {3526},
URL = {https://www.mdpi.com/2072-4292/13/17/3526},
ISSN = {2072-4292},
ABSTRACT = {Crop row detection using unmanned aerial vehicle (UAV) images is very helpful for precision agriculture, enabling one to delineate site-specific management zones and to perform precision weeding. For crop row detection in UAV images, the commonly used Hough transform-based method is not sufficiently accurate. Thus, the purpose of this study is to design a new method for crop row detection in orthomosaic UAV images. For this purpose, nitrogen field experiments involving cotton and nitrogen and water field experiments involving wheat were conducted to create different scenarios for crop rows. During the peak square growth stage of cotton and the jointing growth stage of wheat, multispectral UAV images were acquired. Based on these data, a new crop detection method based on least squares fitting was proposed and compared with a Hough transform-based method that uses the same strategy to preprocess images. The crop row detection accuracy (CRDA) was used to evaluate the performance of the different methods. The results showed that the newly proposed method had CRDA values between 0.99 and 1.00 for different nitrogen levels of cotton and CRDA values between 0.66 and 0.82 for different nitrogen and water levels of wheat. In contrast, the Hough transform method had CRDA values between 0.93 and 0.98 for different nitrogen levels of cotton and CRDA values between 0.31 and 0.53 for different nitrogen and water levels of wheat. Thus, the newly proposed method outperforms the Hough transform method. An effective tool for crop row detection using orthomosaic UAV images is proposed herein.},
DOI = {10.3390/rs13173526}
}


@ARTICLE{RCNN,

  author={Lin, Shaomin and Jiang, Yu and Chen, Xueshen and Biswas, Asim and Li, Shuai and Yuan, Zihao and Wang, Hailin and Qi, Long},
  journal={IEEE Access}, 
  title={Automatic Detection of Plant Rows for a Transplanter in Paddy Field Using Faster R-CNN}, 
  year={2020},
  volume={8},
  number={},
  pages={147231-147240},
  doi={10.1109/ACCESS.2020.3015891}}



@article{LeastSquares2,
title = {Automatic detection of crop rows in maize fields with high weeds pressure},
journal = {Expert Systems with Applications},
volume = {39},
number = {15},
pages = {11889-11897},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.02.117},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412003806},
author = {M. Montalvo and G. Pajares and J.M. Guerrero and J. Romeo and M. Guijarro and A. Ribeiro and J.J. Ruz and J.M. Cruz},
keywords = {Crop row detection, Vegetation index, Image thresholding, Linear regression, Machine vision, Precision agriculture},
abstract = {This paper proposes a new method, oriented to crop row detection in images from maize fields with high weed pressure. The vision system is designed to be installed onboard a mobile agricultural vehicle, i.e. submitted to gyros, vibrations and undesired movements. The images are captured under image perspective, being affected by the above undesired effects. The image processing consists of three main processes: image segmentation, double thresholding, based on the Otsu’s method, and crop row detection. Image segmentation is based on the application of a vegetation index, the double thresholding achieves the separation between weeds and crops and the crop row detection applies least squares linear regression for line adjustment. Crop and weed separation becomes effective and the crop row detection can be favorably compared against the classical approach based on the Hough transform. Both gain effectiveness and accuracy thanks to the double thresholding that makes the main finding of the paper.}
}

@inproceedings{RANSACbase,
  title={An Improved Adaptive Threshold RANSAC Method for Medium Tillage Crop Rows Detection},
  author={Xie, Yinshan and Chen, Kai and Li, Wentao and Zhang, Yan and Mo, Jinqiu},
  booktitle={2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP)},
  pages={1282--1286},
  year={2021},
  organization={IEEE}
}


@article{DeepLearning2,
title = {Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery},
journal = {Computers and Electronics in Agriculture},
volume = {178},
pages = {105766},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105766},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920311376},
author = {Yan Pang and Yeyin Shi and Shancheng Gao and Feng Jiang and Arun-Narenthiran Veeranampalayam-Sivakumar and Laura Thompson and Joe Luck and Chao Liu},
keywords = {Plant population, Deep learning, RCNN, Remote sensing, UAS},
abstract = {Stand counts is one of the most common ways farmers assess plant growth conditions and management practices throughout the season. The conventional method for early-season stand count is through manual inspection, which is time-consuming, laborious, and spatially limited in scope. In recent years, Unmanned Aerial Vehicles (UAV) based remote sensing has been widely used in agriculture to provide low-altitude, high spatial resolution imagery to assist decision making. In this project, we designed a system that uses geometric descriptor information with deep neural networks to determine early-season maize stands from relatively low spatial resolution (10 to 25 mm) aerial data, which covers a relatively large area (10 to 25 hectares). Instead of detecting individual crops in a row, we process the entire row at one time, which significantly reduces the requirements for the clarity of the crops. Besides, our new MaxArea Mask Scoring RCNN algorithm could segment crop-rows out in each patch image, regardless of the terrain conditions. The robustness of our scheme was tested on data collected at two different fields in different years. The accuracy of the estimated emergence rate reached up to 95.8%. Due to the high processing speed of the system, it has the potential for real-time applications in the future.}
}

@InProceedings{LiDARCHangingplantgrowth,
author="Reiser, David
and Miguel, Garrido
and Arellano, Manuel V{\'a}zquez
and Griepentrog, Hans W.
and Paraforos, Dimitris S.",
editor="Reis, Lu{\'i}s Paulo
and Moreira, Ant{\'o}nio Paulo
and Lima, Pedro U.
and Montano, Luis
and Mu{\~{n}}oz-Martinez, Victor",
title="Crop Row Detection in Maize for Developing Navigation Algorithms Under Changing Plant Growth Stages",
booktitle="Robot 2015: Second Iberian Robotics Conference",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="371--382",
abstract="To develop robust algorithms for agricultural navigation, different growth stages of the plants have to be considered. For fast validation and repeatable testing of algorithms, a dataset was recorded by a 4 wheeled robot, equipped with a frame of different sensors and was guided through maize rows. The robot position was simultaneously tracked by a total station, to get precise reference of the sensor data. The plant position and parameters were measured for comparing the sensor values. A horizontal laser scanner and corresponding total station data was recorded for 7 times over a period of 6 weeks. It was used to check the performance of a common RANSAC row algorithm. Results showed the best heading detection at a mean growth height of 0.268 m.",
isbn="978-3-319-27146-0"
}




@ARTICLE{4310076,

  author={Otsu, Nobuyuki},

  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 

  title={A Threshold Selection Method from Gray-Level Histograms}, 

  year={1979},

  volume={9},

  number={1},

  pages={62-66},

  doi={10.1109/TSMC.1979.4310076}}

  
@Article{deepLforSeg,
AUTHOR = {Zhang, Qian and Liu, Yeqi and Gong, Chuanyang and Chen, Yingyi and Yu, Huihui},
TITLE = {Applications of Deep Learning for Dense Scenes Analysis in Agriculture: A Review},
JOURNAL = {Sensors},
VOLUME = {20},
YEAR = {2020},
NUMBER = {5},
ARTICLE-NUMBER = {1520},
URL = {https://www.mdpi.com/1424-8220/20/5/1520},
ISSN = {1424-8220},
ABSTRACT = {Deep Learning (DL) is the state-of-the-art machine learning technology, which shows superior performance in computer vision, bioinformatics, natural language processing, and other areas. Especially as a modern image processing technology, DL has been successfully applied in various tasks, such as object detection, semantic segmentation, and scene analysis. However, with the increase of dense scenes in reality, due to severe occlusions, and small size of objects, the analysis of dense scenes becomes particularly challenging. To overcome these problems, DL recently has been increasingly applied to dense scenes and has begun to be used in dense agricultural scenes. The purpose of this review is to explore the applications of DL for dense scenes analysis in agriculture. In order to better elaborate the topic, we first describe the types of dense scenes in agriculture, as well as the challenges. Next, we introduce various popular deep neural networks used in these dense scenes. Then, the applications of these structures in various agricultural tasks are comprehensively introduced in this review, including recognition and classification, detection, counting and yield estimation. Finally, the surveyed DL applications, limitations and the future work for analysis of dense images in agriculture are summarized.},
DOI = {10.3390/s20051520}
}

@article{HTRTTracking,
title = {Real-Time Tracking of Plant Rows Using a Hough Transform},
journal = {Real-Time Imaging},
volume = {1},
number = {5},
pages = {363-371},
year = {1995},
issn = {1077-2014},
doi = {https://doi.org/10.1006/rtim.1995.1036},
url = {https://www.sciencedirect.com/science/article/pii/S1077201485710364},
author = {John A. Marchant and Renaud Brivot},
abstract = {This paper investigates the problem of deriving vehicle guidance information from images of crops grown in rows. Advantage is taken of the approximately known crop geometry and camera calibration to derive a Hough transform which will detect parallel crop rows. Because the transform integrates information over a number of rows as well as within rows it can operate in a robust fashion with relatively few image features (plant blobs). The algorithm is implemented on one transputer along with some dedicated hardware. The method has been used on six sequences of 30 images and operates at a rate sufficient to give 10 values of vehicle offset and heading angle per second. Typical errors are 12.5 mm of offset and 1.0° of heading angle.}
}

@ARTICLE{7059344,

  author={Bergerman, Marcel and Maeta, Silvio M. and Zhang, Ji and Freitas, Gustavo M. and Hamner, Bradley and Singh, Sanjiv and Kantor, George},

  journal={IEEE Robotics & Automation Magazine}, 

  title={Robot Farmers: Autonomous Orchard Vehicles Help Tree Fruit Production}, 

  year={2015},

  volume={22},

  number={1},

  pages={54-63},

  doi={10.1109/MRA.2014.2369292}}






@ARTICLE{6096039,
  author={Scaramuzza, Davide and Fraundorfer, Friedrich},
  journal={IEEE Robotics & Automation Magazine}, 
  title={Visual Odometry [Tutorial]}, 
  year={2011},
  volume={18},
  number={4},
  pages={80-92},
  doi={10.1109/MRA.2011.943233}}


@Article{AgricultureLiDARZhang,
AUTHOR = {Zhang, Shaolin and Ma, Qianglong and Cheng, Shangkun and An, Dong and Yang, Zhenling and Ma, Biao and Yang, Yang},
TITLE = {Crop Row Detection in the Middle and Late Periods of Maize under Sheltering Based on Solid State LiDAR},
JOURNAL = {Agriculture},
VOLUME = {12},
YEAR = {2022},
NUMBER = {12},
ARTICLE-NUMBER = {2011},
URL = {https://www.mdpi.com/2077-0472/12/12/2011},
ISSN = {2077-0472},
ABSTRACT = {As the basic link of autonomous navigation in agriculture, crop row detection is vital to achieve accurate detection of crop rows for autonomous navigation. Machine vision algorithms are easily affected by factors such as changes in field lighting and weather conditions, and the majority of machine vision algorithms detect early periods of crops, but it is challenging to detect crop rows under high sheltering pressure in the middle and late periods. In this paper, a crop row detection algorithm based on LiDAR is proposed that is aimed at the middle and late crop periods, which has a good effect compared with the conventional machine vision algorithm. The algorithm proposed the following three steps: point cloud preprocessing, feature point extraction, and crop row centerline detection. Firstly, dividing the horizontal strips equally, the improved K-means algorithm and the prior information of the previous horizontal strip are utilized to obtain the candidate points of the current horizontal strip, then the candidate points information is used to filter and extract the feature points in accordance with the corresponding threshold, and finally, the least squares method is used to fit the crop row centerlines. The experimental results show that the algorithm can detect the centerlines of crop rows in the middle and late periods of maize under the high sheltering environment. In the middle period, the average correct extraction rate of maize row centerlines was 95.1%, and the average processing time was 0.181 s; in the late period, the average correct extraction rate of maize row centerlines was 87.3%, and the average processing time was 0.195 s. At the same time, it also demonstrates accuracy and superiority of the algorithm over the machine vision algorithm, which can provide a solid foundation for autonomous navigation in agriculture.},
DOI = {10.3390/agriculture12122011}
}

@InProceedings{CPCalcinHS,
author="Matessi, Andrea
and Lombardi, Luca",
editor="Amestoy, Patrick
and Berger, Philippe
and Dayd{\'e}, Michel
and Ruiz, Daniel
and Duff, Iain
and Frayss{\'e}, Val{\'e}rie
and Giraud, Luc",
title="Vanishing Point Detection in the Hough Transform Space",
booktitle="Euro-Par'99 Parallel Processing",
year="1999",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="987--994",
abstract="Depth estimation from monocular images can be retrieved from the perspective distortion. One major effect of this distortion is that a set of parallel lines in the real world converges into a single point in the image plane. The estimation of the co-ordinates of the vanishing point can be retrieved directly on the Hough Transformation space or polar plane. In fact the vanishing point in the image plane is mapped in the polar plane into a sine curve that can be estimated with a simple linear system.",
isbn="978-3-540-48311-3"
}





@article{Vukobratovic2004ZeroMomentPoint,
author = {Vukobratovi\'{c}, Miomir and Borovac, Branislav},
issn = {0219-8436},
journal = {International Journal of Humanoid Robotics},
number = {01},
pages = {157--173},
publisher = {World Scientific},
title = {{Zero-moment point — thirty five years of its life}},
volume = {1},
year = {2004}
}

@dataset{CRBD,
  author    = {Vidovic et al},
  title     = {Crop Row Benchmark Dataset},
  year      = {2016},
  doi       = {http://www.etfos.unios.hr/r3dvgroup/index.php?id=crd_dataset},
  publisher = {Dryad},
}

